# xpathYtailwind
# Scraper con JavaScript y XPath

Este proyecto permite extraer informaci√≥n de una p√°gina web utilizando JavaScript y expresiones XPath. Funciona correctamente en un entorno local, pero presenta problemas cuando se sube a **GitHub Pages**.

## üö® ¬øPor qu√© no funciona en GitHub Pages?
GitHub Pages es un servicio est√°tico, lo que significa que solo puede servir archivos HTML, CSS y JavaScript. Cuando intentamos hacer `fetch(url)` para obtener el HTML de una p√°gina externa, el navegador bloquea la solicitud debido a las **restricciones de CORS (Cross-Origin Resource Sharing)**. 

Por seguridad, los navegadores no permiten realizar peticiones `fetch` a dominios externos a menos que el servidor de destino lo permita expl√≠citamente con `Access-Control-Allow-Origin: *`.

## üñ•Ô∏è ¬øPor qu√© funciona en local?
Cuando ejecutamos el c√≥digo en local, en muchas ocasiones no se aplican las mismas restricciones de CORS (especialmente si trabajamos con archivos locales y no desde un servidor). Sin embargo, cuando se ejecuta en un dominio como `github.io`, las pol√≠ticas de seguridad del navegador se aplican con m√°s rigor.

## ‚úÖ ¬øC√≥mo solucionarlo?
Aqu√≠ hay algunas soluciones para que el c√≥digo funcione incluso en GitHub Pages:

### 1Ô∏è‚É£ Usar un servidor proxy (CORS Anywhere)
Se puede utilizar un servicio como **CORS Anywhere**, que act√∫a como intermediario entre nuestra aplicaci√≥n y la p√°gina que queremos scrapear.

**Ejemplo:**
```js
const response = await fetch(`https://cors-anywhere.herokuapp.com/${url}`);
```
**Desventaja:** Puede ser lento o tener limitaciones en su uso gratuito.

---

### 2Ô∏è‚É£ Usar una API de terceros
Servicios como [ScraperAPI](https://www.scraperapi.com/) o [ScrapingBee](https://www.scrapingbee.com/) permiten obtener el HTML de una p√°gina web sin restricciones de CORS.

**Ejemplo con ScraperAPI:**
```js
const response = await fetch(`https://api.scraperapi.com?api_key=TU_API_KEY&url=${encodeURIComponent(url)}`);
```
**Desventaja:** Algunos de estos servicios son de pago o tienen l√≠mites gratuitos.

---

### 3Ô∏è‚É£ Hacer el scraping desde el backend
Si tienes acceso a un servidor backend (por ejemplo, con PHP o Laravel), puedes hacer la petici√≥n desde ah√≠ y devolver el HTML a tu frontend en GitHub Pages.

**Ejemplo con PHP:**
```php
<?php
header('Access-Control-Allow-Origin: *');
$url = $_GET['url'] ?? '';
if (filter_var($url, FILTER_VALIDATE_URL)) {
    echo file_get_contents($url);
} else {
    echo 'URL no v√°lida';
}
?>
```
Luego, en JavaScript:
```js
const response = await fetch(`https://tu-servidor.com/scraper.php?url=${encodeURIComponent(url)}`);
```
**Desventaja:** Necesitas un servidor propio.

---

## üöÄ Conclusi√≥n
Si quieres que el scraper funcione en GitHub Pages, necesitas usar una de las soluciones mencionadas. La mejor opci√≥n depender√° de tus necesidades y recursos. Si solo est√°s probando, CORS Anywhere puede servir. Para un proyecto m√°s estable, es recomendable un backend propio o una API de scraping.

üìå **Recuerda:** Hacer scraping en algunas p√°ginas puede estar restringido por sus t√©rminos de uso. ¬°√ösalo con responsabilidad! üòâ
